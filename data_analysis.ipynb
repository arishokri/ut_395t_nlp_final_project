{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import textwrap\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pretty-print JSON with wrapped lines\n",
    "def pretty_print_json(data, max_width=80):\n",
    "    json_str = json.dumps(data, indent=4)\n",
    "    wrapped_lines = []\n",
    "    for line in json_str.splitlines():\n",
    "        wrapped_lines.extend(textwrap.wrap(line, width=max_width))\n",
    "    return \"\\n\".join(wrapped_lines)\n",
    "\n",
    "file_path = os.path.join(\"eval_baseline_emrqa\", \"eval_predictions.jsonl\")\n",
    "with open(file_path, \"r\") as file:\n",
    "    predictions = [json.loads(line) for line in file]\n",
    "\n",
    "# Filter failed examples where the predicted answer does not match the expected answer\n",
    "failed_examples = [\n",
    "    prediction for prediction in predictions\n",
    "    if prediction['predicted_answer'] != prediction['answers']['text'][0] # for whatever reason this is a list\n",
    "]\n",
    "\n",
    "# Put Failed examples in a dataframe\n",
    "failed = pd.DataFrame(failed_examples)\n",
    "failed['answer'] = [v['text'][0] for v in failed[\"answers\"].values]\n",
    "# pd.set_option('display.max_colwidth', 20)\n",
    "failed.columns\n",
    "len(failed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Rule Based Analysis based on Span and character matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Basic helpers\n",
    "# -----------------------------\n",
    "\n",
    "STOPWORDS = [\n",
    "    \"the\",\"a\",\"an\",\"and\",\"or\",\"but\",\"if\",\"then\",\"so\",\"for\",\"of\",\"on\",\"in\",\"to\",\n",
    "    \"with\",\"without\",\"by\",\"at\",\"from\",\"as\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n",
    "    \"being\",\"that\",\"this\",\"these\",\"those\",\"it\",\"its\",\"into\",\"about\",\"than\",\n",
    "    \"such\",\"which\",\"who\",\"whom\",\"whose\",\"what\",\"when\",\"where\",\"why\",\"how\"\n",
    "]\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"\\w+|\\d+|\\S\", text.lower())\n",
    "\n",
    "def content_tokens(text):\n",
    "    toks = tokenize(text)\n",
    "    return [t for t in toks if t.isalpha() and t not in STOPWORDS]\n",
    "\n",
    "def jaccard_overlap(a, b):\n",
    "    A = set(content_tokens(a))\n",
    "    B = set(content_tokens(b))\n",
    "    if not A or not B:\n",
    "        return 0.0\n",
    "    return len(A & B) / len(A | B)\n",
    "\n",
    "def has_number(text):\n",
    "    return bool(re.search(r\"\\d\", text))\n",
    "\n",
    "def count_numbers(text):\n",
    "    return len(re.findall(r\"\\d+(?:\\.\\d+)?\", text))\n",
    "\n",
    "MONTH_PAT = r\"(jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec)\"\n",
    "DATE_PAT = rf\"(\\d{{1,2}}/\\d{{1,2}}/\\d{{2,4}}|\\d{{4}}-\\d{{1,2}}-\\d{{1,2}}|{MONTH_PAT})\"\n",
    "\n",
    "def count_dates(text):\n",
    "    return len(re.findall(DATE_PAT, text.lower()))\n",
    "\n",
    "def shannon_entropy(tokens):\n",
    "    if not tokens:\n",
    "        return 0.0\n",
    "    counts = Counter(tokens)\n",
    "    total = sum(counts.values())\n",
    "    probs = [c/total for c in counts.values()]\n",
    "    return -sum(p*np.log(p + 1e-12) for p in probs)\n",
    "\n",
    "def stopword_ratio(text):\n",
    "    toks = tokenize(text)\n",
    "    if not toks:\n",
    "        return 0.0\n",
    "    sw = sum(1 for t in toks if t in STOPWORDS)\n",
    "    return sw / len(toks)\n",
    "\n",
    "def classify_question_type(q):\n",
    "    q_low = q.lower()\n",
    "    if any(x in q_low for x in [\"when\", \"date\", \"time\", \"year\", \"day\", \"month\"]):\n",
    "        return \"time\"\n",
    "    if any(x in q_low for x in [\"how many\", \"how much\", \"number\", \"count\", \"dose\", \"dosage\", \"mg\", \"mcg\"]):\n",
    "        return \"number\"\n",
    "    return \"other\"\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Global stats for thresholds (lengths)\n",
    "# -----------------------------\n",
    "# Compute gold answer lengths for failed examples\n",
    "gold_lengths = failed['answer'].fillna(\"\").apply(lambda x: len(tokenize(x)))\n",
    "\n",
    "# 95th percentile of gold answer lengths (for anomaly thresholding)\n",
    "length_p95 = np.percentile(gold_lengths, 95) if len(gold_lengths) > 0 else 100\n",
    "\n",
    "# Median gold answer length (typical answer size)\n",
    "length_med = np.median(gold_lengths) if len(gold_lengths) > 0 else 0\n",
    "\n",
    "# Median absolute deviation (robust spread/variability)\n",
    "mad = np.median(np.abs(gold_lengths - length_med)) if len(gold_lengths) > 0 else 0\n",
    "\n",
    "# Tune this as needed\n",
    "LENGTH_THRESHOLD = max(length_p95, length_med + 2 * mad)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Rule implementations (per-row)\n",
    "# -----------------------------\n",
    "\n",
    "def rule1_length_anomaly(answer_text):\n",
    "    \"\"\"Gold span length is abnormally large.\"\"\"\n",
    "    return len(tokenize(answer_text)) > LENGTH_THRESHOLD\n",
    "\n",
    "def rule2_multi_clause(answer_text):\n",
    "    \"\"\"Gold span looks like multiple clauses/list items.\"\"\"\n",
    "    clauses = re.split(r',|\\n|;|\\band\\b|\\bor\\b', answer_text, flags=re.IGNORECASE)\n",
    "    informative = [c for c in clauses if len(content_tokens(c)) >= 3]\n",
    "    return len(informative) > 1\n",
    "\n",
    "def rule3_low_question_similarity(answer_text, question, sim_threshold=0.05):\n",
    "    \"\"\"Gold span has very low lexical overlap with question.\"\"\"\n",
    "    overlap = jaccard_overlap(answer_text, question)\n",
    "    return overlap < sim_threshold\n",
    "\n",
    "def rule4_pred_inside_gold_better_alignment(pred, gold, question, margin=0.1):\n",
    "    \"\"\"\n",
    "    Predicted span is inside gold span AND aligns better with the question.\n",
    "    Strong hint gold is a noisy chunk. This is a very important one.\n",
    "    \"\"\"\n",
    "    gold_clean = gold or \"\"\n",
    "    pred_clean = pred or \"\"\n",
    "    if not pred_clean or pred_clean.strip() not in gold_clean:\n",
    "        return False\n",
    "    q_gold = jaccard_overlap(gold_clean, question)\n",
    "    q_pred = jaccard_overlap(pred_clean, question)\n",
    "    return q_pred > q_gold + margin\n",
    "\n",
    "def rule5_question_type_mismatch(answer_text, question):\n",
    "    \"\"\"\n",
    "    Question type vs answer structure mismatch:\n",
    "    - time question but zero or multiple date-like expressions\n",
    "    - number question but zero or many numeric expressions\n",
    "    \"\"\"\n",
    "    qtype = classify_question_type(question)\n",
    "    if qtype == \"time\":\n",
    "        n_dates = count_dates(answer_text)\n",
    "        return n_dates == 0 or n_dates > 1\n",
    "    if qtype == \"number\":\n",
    "        n_nums = count_numbers(answer_text)\n",
    "        return n_nums == 0 or n_nums > 3  # >3 is \"probably a list\"\n",
    "    return False\n",
    "\n",
    "def rule6_multiple_occurrences_in_context(answer_text, context):\n",
    "    \"\"\"Gold span appears multiple times in the context: ambiguous / alignment suspect.\"\"\"\n",
    "    if not answer_text:\n",
    "        return False\n",
    "    return context.count(answer_text) > 1\n",
    "\n",
    "def rule7_boundary_weirdness(answer_text):\n",
    "    \"\"\"Gold span starts/ends with separators or looks cut mid-clause.\"\"\"\n",
    "    txt = answer_text.strip()\n",
    "    if not txt:\n",
    "        return False\n",
    "    if txt.startswith((\",\", \";\", \"and\", \"or\")):\n",
    "        return True\n",
    "    if txt.endswith((\",\", \";\", \"and\", \"or\")):\n",
    "        return True\n",
    "    # starts/ends with odd characters\n",
    "    if txt[0] in \"/-:\" or txt[-1] in \"/-:\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def rule8_pred_answers_question_better(pred, gold, question, margin=0.1):\n",
    "    \"\"\"Pred span has much better question alignment than gold (even if not substring).\"\"\"\n",
    "    q_gold = jaccard_overlap(gold or \"\", question)\n",
    "    q_pred = jaccard_overlap(pred or \"\", question)\n",
    "    return q_pred > q_gold + margin\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Apply rules to `failed` dataframe\n",
    "# -----------------------------\n",
    "\n",
    "def compute_dataset_error_flags(row):\n",
    "    ctx = row.get(\"context\", \"\") or \"\"\n",
    "    q   = row.get(\"question\", \"\") or \"\"\n",
    "    gold = row.get(\"answer\", \"\") or \"\"\n",
    "    pred = row.get(\"predicted_answer\", \"\") or \"\"\n",
    "\n",
    "    r1 = rule1_length_anomaly(gold)\n",
    "    r2 = rule2_multi_clause(gold)\n",
    "    r3 = rule3_low_question_similarity(gold, q)\n",
    "    r4 = rule4_pred_inside_gold_better_alignment(pred, gold, q)\n",
    "    r5 = rule5_question_type_mismatch(gold, q)\n",
    "    r6 = rule6_multiple_occurrences_in_context(gold, ctx)\n",
    "    r7 = rule7_boundary_weirdness(gold)\n",
    "    r8 = rule8_pred_answers_question_better(pred, gold, q)\n",
    "\n",
    "    flags = {\n",
    "        \"rule1_length_anomaly\": r1,\n",
    "        \"rule2_multi_clause\": r2,\n",
    "        \"rule3_low_q_similarity\": r3,\n",
    "        \"rule4_pred_inside_gold_better\": r4,\n",
    "        \"rule5_qtype_mismatch\": r5,\n",
    "        \"rule6_multi_occurrences\": r6,\n",
    "        \"rule7_boundary_weirdness\": r7,\n",
    "        \"rule8_pred_better_q_alignment\": r8,\n",
    "    }\n",
    "    # simple score: how many rules think \"dataset error\"\n",
    "    flags[\"dataset_error_score\"] = sum(flags.values())\n",
    "    # threshold: at least 2 rules triggered -> call it dataset error (tune this)\n",
    "    flags[\"is_dataset_error\"] = flags[\"dataset_error_score\"] >= 2\n",
    "    return pd.Series(flags)\n",
    "\n",
    "# Apply to all failed examples\n",
    "error_flags = failed.apply(compute_dataset_error_flags, axis=1)\n",
    "failed_with_flags = pd.concat([failed, error_flags], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5. Quick sanity checks / summaries\n",
    "# -----------------------------\n",
    "\n",
    "print(\"Total failed examples:\", len(failed_with_flags))\n",
    "print(\"Flagged as dataset errors:\", failed_with_flags[\"is_dataset_error\"].sum())\n",
    "\n",
    "print(\"\\nRule trigger rates:\")\n",
    "print(failed_with_flags[\n",
    "    [c for c in failed_with_flags.columns if c.startswith(\"rule\")]\n",
    "].sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify rule columns\n",
    "rule_cols = [c for c in failed_with_flags.columns if c.startswith(\"rule\")]\n",
    "\n",
    "# Compute summary\n",
    "summary = pd.DataFrame({\n",
    "    \"count\": failed_with_flags[rule_cols].sum(),\n",
    "    \"percent\": failed_with_flags[rule_cols].mean() * 100\n",
    "}).sort_values(\"count\", ascending=False)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "summary[\"percent\"].plot(kind=\"bar\", color=\"blue\")\n",
    "plt.title(\"Dataset Error Rule Trigger Percentages\")\n",
    "plt.ylabel(\"Percentage (%)\")\n",
    "plt.xlabel(\"Rule\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = failed_with_flags[failed_with_flags[\"rule2_multi_clause\"]]\n",
    "r7 = failed_with_flags[failed_with_flags[\"rule7_boundary_weirdness\"]]\n",
    "counts = pd.Series({\n",
    "    \"Multi-Clause (Rule 2)\": r2.shape[0],\n",
    "    \"Boundary Weirdness (Rule 7)\": r7.shape[0]\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "counts.plot(kind=\"bar\", color=[\"green\", \"pink\"])\n",
    "plt.title(\"Major Dataset Artifact Types\")\n",
    "plt.ylabel(\"Number of Affected Examples\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis=\"y\", alpha=0.4, linestyle=\"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(failed[failed['predicted_answer'] == ',']), len(failed[failed['predicted_answer'] == '.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "###  Baseline EMRQA prediction error analysis\n",
    "- High-level error patterns for `eval_baseline_emrqa/eval_predictions.jsonl`\n",
    "- Label wrong predictions as `truncated_span`, `overlong_span`, `partial_overlap`, or `no_overlap` and show counts/examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "token_re = re.compile(r\"\\w+\")\n",
    "relation = [\"truncated_span\", \"overrun_span\", \"partial_overlap\", \"no_overlap\"]\n",
    "\n",
    "def norm(text):\n",
    "    return \" \".join(text.lower().split())\n",
    "\n",
    "def tokens(text: str):\n",
    "    return token_re.findall(text.lower())\n",
    "\n",
    "def span_relation(prediction: str, actual: str):\n",
    "    p = norm(prediction)\n",
    "    a = norm(actual)\n",
    "    if p and p in a:\n",
    "        return relation[0]\n",
    "    if a and a in p:\n",
    "        return relation[1]\n",
    "    if set(tokens(p)) & set(tokens(a)):\n",
    "        return relation[2]\n",
    "    return relation[3]\n",
    "\n",
    "# Add a new column to the failed dataframe for the span relation\n",
    "failed['relation'] = failed.apply(\n",
    "    lambda row: span_relation(\n",
    "        row.get(\"predicted_answer\", \"\"),\n",
    "        row.get(\"answer\", \"\")\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Display counts from the failed dataframe\n",
    "print(f\"Total wrong predictions: {len(failed)}\")\n",
    "print(\"Wrong predictions by type:\")\n",
    "print(failed[\"relation\"].value_counts())\n",
    "\n",
    "# Use the following filter to deep dive\n",
    "# Sample examples from each relation type\n",
    "samples = []\n",
    "for rel in relation:\n",
    "    rel_samples = failed[failed[\"relation\"] == rel][[\"id\", \"question\", \"predicted_answer\", \"answer\", \"relation\"]].head(3)\n",
    "    samples.append(rel_samples)\n",
    "\n",
    "pd.concat(samples, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Deep Dive into Baseline EMRQA Prediction Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Cluster error types in failed examples for further analysis. This uses sentence transformers to embed the question, context, and predicted/true answers, then clusters them using KMeans. This seems to work better. However, the clusters are still not very meaningful. Further work is needed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed['gold_norm'] = failed['answer'].apply(norm)\n",
    "failed['pred_norm'] = failed['predicted_answer'].apply(norm)\n",
    "failed[\"clust_text\"] = failed.apply(\n",
    "    lambda r: f\"Q: {r['question']} GOLD: {r['gold_norm']} PRED: {r['pred_norm']}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "embed_texts = failed[\"clust_text\"].tolist()\n",
    "embeddings = model.encode(\n",
    "    embed_texts,\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "embeddings.shape\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pca_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "failed[\"pca_x\"] = pca_2d[:, 0]\n",
    "failed[\"pca_y\"] = pca_2d[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = list(range(2, 16))\n",
    "sil_scores = []\n",
    "\n",
    "for k in ks:\n",
    "    km = KMeans(\n",
    "        n_clusters=k,\n",
    "        random_state=0,\n",
    "        n_init=10,\n",
    "        max_iter=300\n",
    "    )\n",
    "    labels = km.fit_predict(embeddings)\n",
    "    score = silhouette_score(embeddings, labels)\n",
    "    sil_scores.append(score)\n",
    "    print(f\"k={k:2d}  silhouette={score:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(ks, sil_scores, marker=\"o\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Silhouette score\")\n",
    "plt.title(\"Silhouette scores (KMeans on embeddings)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = 2\n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=best_k,\n",
    "    random_state=0,\n",
    "    n_init=10,\n",
    "    max_iter=300\n",
    ")\n",
    "k_labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "failed[\"cluster_kmeans\"] = k_labels\n",
    "failed[\"cluster_kmeans\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for cid in sorted(failed[\"cluster_kmeans\"].unique()):\n",
    "    mask = failed[\"cluster_kmeans\"] == cid\n",
    "    plt.scatter(\n",
    "        failed.loc[mask, \"pca_x\"],\n",
    "        failed.loc[mask, \"pca_y\"],\n",
    "        s=5,\n",
    "        alpha=0.7,\n",
    "        label=f\"cluster {cid}\"\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"PCA-1\")\n",
    "plt.ylabel(\"PCA-2\")\n",
    "plt.title(f\"KMeans (k={best_k}) on embeddings (PCA 2D)\")\n",
    "plt.legend(markerscale=3, bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_k_info = {}\n",
    "\n",
    "for cid in sorted(failed[\"cluster_kmeans\"].unique()):\n",
    "    sub = failed[failed[\"cluster_kmeans\"] == cid]\n",
    "    n = len(sub)\n",
    "    rel_dist = sub[\"relation\"].value_counts().to_dict()\n",
    "    \n",
    "    top_terms = []\n",
    "    if n >= 5:\n",
    "        cv = CountVectorizer(\n",
    "            max_features=2000,\n",
    "            ngram_range=(1,2),\n",
    "            stop_words=\"english\"\n",
    "        )\n",
    "        Xq = cv.fit_transform(sub[\"question\"].astype(str).tolist())\n",
    "        sums = np.asarray(Xq.sum(axis=0)).ravel()\n",
    "        terms = np.array(cv.get_feature_names_out())\n",
    "        top_idx = sums.argsort()[::-1][:15]\n",
    "        top_terms = list(terms[top_idx])\n",
    "    \n",
    "    cluster_k_info[cid] = {\n",
    "        \"n\": n,\n",
    "        \"relation_dist\": rel_dist,\n",
    "        \"top_terms\": top_terms,\n",
    "    }\n",
    "\n",
    "for cid, info in cluster_k_info.items():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Cluster {cid}  |  n = {info['n']}\")\n",
    "    print(\"Error-type distribution:\", info[\"relation_dist\"])\n",
    "    print(\"Top question terms:\", \", \".join(info[\"top_terms\"]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Manual Review of Sampled Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "- No. of questions in the train dataset that start with \"has\" is 62872/130,956\n",
    "- No. of questions in the validation dataset that start with \"has\" is 15913/32739\n",
    "- No. of questions in the failed examples that start with \"has\" is 1423/3248"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "This examines the whole dataset for train and eval to see how many questions start with \"has\", 'why' etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_dataset(\"Eladio/emrqa-msquad\", split=\"train\")\n",
    "\n",
    "# Filter questions that start with \"why\"\n",
    "why_questions = train.filter(lambda example: example[\"question\"].lower().startswith(\"why\"))\n",
    "\n",
    "why_questions_df = pd.DataFrame(why_questions)\n",
    "len(why_questions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = load_dataset(\"Eladio/emrqa-msquad\", split=\"validation\")\n",
    "\n",
    "# Filter questions that start with \"has\"\n",
    "has_questions = eval.filter(lambda example: example[\"question\"].lower().startswith(\"has\"))\n",
    "\n",
    "# Display the filtered examples\n",
    "has_questions_df = pd.DataFrame(has_questions)\n",
    "has_questions_df['question']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Insulin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "insulin_questions = failed[failed['question'].str.contains('insulin', case=False, na=False)]\n",
    "len(insulin_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "Chest pain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "chest_pain_questions = failed[failed['question'].str.contains('chest pain', case=False, na=False)]\n",
    "len(chest_pain_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Medication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "medication_questions = failed[failed['question'].str.contains('medication', case=False, na=False)]\n",
    "len(chest_pain_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Gemfibrozil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemfibrozil_examples = failed[failed['question'].str.lower().str.contains('gemfibrozil')]\n",
    "gemfibrozil_examples.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Ignore for the time being\n",
    "Tfidf + k-means did not yield meaningful clusters here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=3000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(failed[\"clust_text\"])\n",
    "# X.shape\n",
    "k = 10\n",
    "kmeans = KMeans(n_clusters=k, random_state=0, n_init=10, max_iter=100)\n",
    "cluster_labels = kmeans.fit_predict(X)\n",
    "\n",
    "failed[\"cluster\"] = cluster_labels\n",
    "cluster_sizes = failed[\"cluster\"].value_counts().sort_index()\n",
    "# cluster_sizes\n",
    "\n",
    "# per cluster analysis\n",
    "cluster_summaries = {}\n",
    "\n",
    "for c in sorted(failed[\"cluster\"].unique()):\n",
    "    sub = failed[failed[\"cluster\"] == c]\n",
    "\n",
    "    qs = sub[\"question\"].astype(str).tolist()\n",
    "    if len(qs) < 5:\n",
    "        continue\n",
    "\n",
    "    cv = CountVectorizer(\n",
    "        max_features=2000,\n",
    "        ngram_range=(1,3),\n",
    "        stop_words=\"english\"\n",
    "    )\n",
    "    Xq = cv.fit_transform(qs)\n",
    "\n",
    "    sums = np.asarray(Xq.sum(axis=0)).ravel()\n",
    "    terms = np.array(cv.get_feature_names_out())\n",
    "    top_idx = sums.argsort()[::-1][:15]\n",
    "\n",
    "    cluster_summaries[c] = {\n",
    "        \"n\": len(sub),\n",
    "        \"top_terms\": list(terms[top_idx]),\n",
    "        \"error_dist\": sub[\"relation\"].value_counts().to_dict()\n",
    "    }\n",
    "\n",
    "# cluster_summaries\n",
    "\n",
    "for c in sorted(cluster_summaries.keys()):\n",
    "    info = cluster_summaries[c]\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Cluster {c}  |  n = {info['n']}\")\n",
    "    print(\"Top terms:\", \", \".join(info[\"top_terms\"]))\n",
    "    print(\"Error types:\", info[\"error_dist\"])\n",
    "    print()\n",
    "\n",
    "    ks = list(range(2, 25))\n",
    "sse = []\n",
    "\n",
    "# find the right k\n",
    "for k in ks:\n",
    "    km = KMeans(n_clusters=k, random_state=0, n_init=10, max_iter=200)\n",
    "    km.fit(X)\n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "plt.plot(ks, sse, marker='o')\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Inertia (SSE)\")\n",
    "plt.title(\"Elbow Method\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final-project (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
