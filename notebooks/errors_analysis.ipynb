{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Basic Setup and Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# SETUP AND CONFIGURATION\n",
    "# ==================================================================================\n",
    "\n",
    "# Standard libraries\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import ast\n",
    "from collections import Counter\n",
    "\n",
    "# Data science libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Custom modules\n",
    "sys.path.append('..')\n",
    "from errors_analysis import *\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "# ==================================================================================\n",
    "\n",
    "def load_predictions(file_path):\n",
    "    \"\"\"\n",
    "    Load prediction results from a JSONL file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the JSONL file containing predictions\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list : List of prediction dictionaries\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return [json.loads(line) for line in file]\n",
    "\n",
    "\n",
    "def extract_failed_predictions(predictions):\n",
    "    \"\"\"\n",
    "    Extract and format failed predictions into a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : list\n",
    "        List of prediction dictionaries\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : DataFrame containing failed predictions with extracted answer column\n",
    "    \"\"\"\n",
    "    # Filter failed examples\n",
    "    failed_examples = [\n",
    "        pred for pred in predictions\n",
    "        if pred['predicted_answer'] != pred['answers']['text'][0]\n",
    "    ]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    failed_df = pd.DataFrame(failed_examples)\n",
    "    \n",
    "    # Extract answer text from nested structure\n",
    "    failed_df['answer'] = [v['text'][0] for v in failed_df['answers'].values]\n",
    "    \n",
    "    return failed_df\n",
    "\n",
    "\n",
    "def print_dataset_summary(predictions, failed_df):\n",
    "    \"\"\"\n",
    "    Print summary statistics for the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : list\n",
    "        Full list of predictions\n",
    "    failed_df : pd.DataFrame\n",
    "        DataFrame of failed predictions\n",
    "    \"\"\"\n",
    "    total = len(predictions)\n",
    "    failed = len(failed_df)\n",
    "    accuracy = (1 - failed/total) * 100\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"DATASET SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Total predictions:  {total:,}\")\n",
    "    print(f\"Failed predictions: {failed:,}\")\n",
    "    print(f\"Success rate:       {total - failed:,}\")\n",
    "    print(f\"Accuracy:           {accuracy:.2f}%\")\n",
    "    print(f\"\\nDataFrame shape:    {failed_df.shape}\")\n",
    "    print(f\"Columns:            {list(failed_df.columns)}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# ==================================================================================\n",
    "# MAIN EXECUTION\n",
    "# ==================================================================================\n",
    "\n",
    "# Load predictions from trained model\n",
    "predictions_file = os.path.join(\"../trained_models/train_rule_cluster_val\", \"eval_predictions.jsonl\")\n",
    "predictions = load_predictions(predictions_file)\n",
    "\n",
    "# Extract failed predictions\n",
    "failed = extract_failed_predictions(predictions)\n",
    "\n",
    "# Display summary\n",
    "print_dataset_summary(predictions, failed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Rule Based Analysis based on Span and character matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "The following analysis is the updated errors_analysis.py to file to do the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# RULE-BASED ERROR DETECTION\n",
    "# ==================================================================================\n",
    "\n",
    "# Apply rule-based error detection to identify dataset quality issues\n",
    "df_with_flags, summary = detect_dataset_errors(\n",
    "    df=failed,\n",
    "    error_threshold=2,           # minimum rules to trigger for dataset error\n",
    "    length_percentile=95         # percentile for length anomaly detection\n",
    ")\n",
    "\n",
    "# Display rule trigger summary\n",
    "print(\"=\" * 80)\n",
    "print(\"RULE TRIGGER SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(summary)\n",
    "print()\n",
    "\n",
    "# Extract and display dataset errors\n",
    "dataset_errors = df_with_flags[df_with_flags['is_dataset_error']]\n",
    "print(f\"Found {len(dataset_errors)} potential dataset errors\")\n",
    "print(f\"Dataset error rate: {len(dataset_errors)/len(failed)*100:.1f}%\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df_with_flags[df_with_flags['rule13_model_wrong_entity_same_type']][['question', 'answer', 'predicted_answer', 'rule12_model_underextraction']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_flags.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Deeper Analysis based on combining the rules for a better story"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "We combine the rules to get a better understanding of the errors and generalize the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# ENHANCED ERROR ANALYSIS - COMPOSITE CATEGORIES\n",
    "# ==================================================================================\n",
    "\n",
    "# Apply enhanced error analysis to combine rules into meaningful categories\n",
    "df_enhanced, composite_summary, medical_summary, model_behavior_summary = enhanced_error_analysis(df_with_flags)\n",
    "\n",
    "# Display all error category summaries\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPOSITE ERROR CATEGORIES\")\n",
    "print(\"=\" * 80)\n",
    "print(composite_summary)\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MEDICAL DOMAIN-SPECIFIC ERRORS\")\n",
    "print(\"=\" * 80)\n",
    "print(medical_summary)\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL BEHAVIORAL PATTERNS\")\n",
    "print(\"=\" * 80)\n",
    "print(model_behavior_summary)\n",
    "print()\n",
    "\n",
    "# Summary statistics\n",
    "total_examples = len(df_enhanced)\n",
    "dataset_errors = df_enhanced['is_dataset_error'].sum()\n",
    "medical_errors = df_enhanced[['med_semantic_error', 'vague_question_error', 'frequency_error']].any(axis=1).sum()\n",
    "model_errors = df_enhanced['has_model_behavior_error'].sum()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"OVERALL ERROR DISTRIBUTION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total examples analyzed:        {total_examples:,}\")\n",
    "print(f\"Dataset quality issues:         {dataset_errors:,} ({dataset_errors/total_examples*100:.1f}%)\")\n",
    "print(f\"Medical domain errors:          {medical_errors:,} ({medical_errors/total_examples*100:.1f}%)\")\n",
    "print(f\"Model behavioral errors:        {model_errors:,} ({model_errors/total_examples*100:.1f}%)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Show representative examples from each major category\n",
    "print(\"=\" * 80)\n",
    "print(\"REPRESENTATIVE EXAMPLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Semantic mismatch examples\n",
    "print(\"\\n1. SEMANTIC MISMATCH (Most Common Dataset Issue)\")\n",
    "print(\"-\" * 80)\n",
    "semantic_examples = df_enhanced[df_enhanced['semantic_mismatch']].head(2)\n",
    "for idx, row in semantic_examples.iterrows():\n",
    "    print(f\"\\nQuestion:   {row['question'][:70]}...\")\n",
    "    print(f\"Gold:       {row['answer'][:70]}...\")\n",
    "    print(f\"Predicted:  {row['predicted_answer'][:70]}...\")\n",
    "    print(f\"Error Score: {row['dataset_error_score']}\")\n",
    "\n",
    "# 2. Medical semantic errors\n",
    "print(\"\\n\\n2. MEDICAL SEMANTIC ERRORS\")\n",
    "print(\"-\" * 80)\n",
    "med_examples = df_enhanced[df_enhanced['med_semantic_error']].head(2)\n",
    "for idx, row in med_examples.iterrows():\n",
    "    print(f\"\\nQuestion:   {row['question'][:70]}...\")\n",
    "    print(f\"Gold:       {row['answer'][:70]}...\")\n",
    "    print(f\"Predicted:  {row['predicted_answer'][:70]}...\")\n",
    "\n",
    "# 3. Model underextraction\n",
    "print(\"\\n\\n3. MODEL UNDEREXTRACTION (Stopped Too Early)\")\n",
    "print(\"-\" * 80)\n",
    "underextract_examples = df_enhanced[df_enhanced['model_underextraction_pattern']].head(2)\n",
    "for idx, row in underextract_examples.iterrows():\n",
    "    print(f\"\\nQuestion:   {row['question'][:70]}...\")\n",
    "    print(f\"Gold:       {row['answer'][:70]}...\")\n",
    "    print(f\"Predicted:  {row['predicted_answer'][:70]}...\")\n",
    "    print(f\"Length:     {len(row['predicted_answer'])} chars vs {len(row['answer'])} chars (gold)\")\n",
    "\n",
    "# 4. Entity confusion\n",
    "print(\"\\n\\n4. MODEL ENTITY CONFUSION (Wrong Entity, Same Type)\")\n",
    "print(\"-\" * 80)\n",
    "entity_examples = df_enhanced[df_enhanced['model_entity_confusion_pattern']].head(2)\n",
    "for idx, row in entity_examples.iterrows():\n",
    "    print(f\"\\nQuestion:   {row['question'][:70]}...\")\n",
    "    print(f\"Gold:       {row['answer'][:70]}...\")\n",
    "    print(f\"Predicted:  {row['predicted_answer'][:70]}...\")\n",
    "\n",
    "# 5. Position bias\n",
    "print(\"\\n\\n5. MODEL POSITION BIAS (Wrong Location in Context)\")\n",
    "print(\"-\" * 80)\n",
    "position_examples = df_enhanced[df_enhanced['model_position_bias_pattern']].head(2)\n",
    "for idx, row in position_examples.iterrows():\n",
    "    print(f\"\\nQuestion:   {row['question'][:70]}...\")\n",
    "    print(f\"Gold:       {row['answer'][:70]}...\")\n",
    "    print(f\"Predicted:  {row['predicted_answer'][:70]}...\")\n",
    "\n",
    "# 6. Boundary failure\n",
    "print(\"\\n\\n6. MODEL BOUNDARY FAILURE (Stopped at Punctuation)\")\n",
    "print(\"-\" * 80)\n",
    "boundary_examples = df_enhanced[df_enhanced['model_boundary_failure_pattern']].head(2)\n",
    "for idx, row in boundary_examples.iterrows():\n",
    "    print(f\"\\nQuestion:   {row['question'][:70]}...\")\n",
    "    print(f\"Gold:       {row['answer'][:70]}...\")\n",
    "    print(f\"Predicted:  {row['predicted_answer'][:70]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Visualization of Composite Error Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations for composite error categories\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Composite error categories breakdown\n",
    "composite_summary.plot(kind='bar', y='count', ax=ax1, color='steelblue', legend=False)\n",
    "ax1.set_title('Composite Error Categories - Count', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Number of Examples')\n",
    "ax1.set_xlabel('Category')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Medical domain-specific errors\n",
    "medical_summary.plot(kind='bar', y='count', ax=ax2, color='coral', legend=False)\n",
    "ax2.set_title('Medical Domain-Specific Errors - Count', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Number of Examples')\n",
    "ax2.set_xlabel('Error Type')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Error precision for composite categories\n",
    "composite_summary.plot(kind='bar', y='error_precision', ax=ax3, color='green', legend=False)\n",
    "ax3.set_title('Composite Categories - Error Precision', fontsize=14, fontweight='bold')\n",
    "ax3.set_ylabel('Precision (%)')\n",
    "ax3.set_xlabel('Category')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.axhline(y=50, color='red', linestyle='--', alpha=0.7, label='50% threshold')\n",
    "ax3.legend()\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Medical error precision\n",
    "medical_summary.plot(kind='bar', y='error_precision', ax=ax4, color='purple', legend=False)\n",
    "ax4.set_title('Medical Errors - Error Precision', fontsize=14, fontweight='bold')\n",
    "ax4.set_ylabel('Precision (%)')\n",
    "ax4.set_xlabel('Error Type')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "ax4.axhline(y=50, color='red', linestyle='--', alpha=0.7, label='50% threshold')\n",
    "ax4.legend()\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print key statistics\n",
    "print(\"=== KEY STATISTICS ===\")\n",
    "print(f\"Total examples: {len(df_enhanced)}\")\n",
    "print(f\"Flagged as dataset errors: {df_enhanced['is_dataset_error'].sum()} ({df_enhanced['is_dataset_error'].mean()*100:.1f}%)\")\n",
    "print(f\"\\nSemantic mismatch (primary issue): {df_enhanced['semantic_mismatch'].sum()} ({df_enhanced['semantic_mismatch'].mean()*100:.1f}%)\")\n",
    "print(f\"Medical-specific errors: {df_enhanced['med_semantic_error'].sum()} ({df_enhanced['med_semantic_error'].mean()*100:.1f}%)\")\n",
    "print(f\"Template generation issues: {df_enhanced['template_artifacts'].sum()} ({df_enhanced['template_artifacts'].mean()*100:.1f}%)\")\n",
    "print(f\"\\nSemantic mismatch precision for dataset errors: {df_enhanced[df_enhanced['semantic_mismatch']]['is_dataset_error'].mean()*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Medical Domain Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze medical domain-specific patterns in detail\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Medication error breakdown\n",
    "medication_cols = ['med_semantic_error', 'frequency_error']\n",
    "med_counts = df_enhanced[medication_cols].sum()\n",
    "ax1.bar(range(len(med_counts)), med_counts.values, color=['#FF6B6B', '#FFA07A'])\n",
    "ax1.set_title('Medication-Related Errors', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Number of Examples')\n",
    "ax1.set_xticks(range(len(med_counts)))\n",
    "ax1.set_xticklabels(['All Med Errors', 'Frequency Errors'], rotation=0)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Vague question analysis\n",
    "vague_count = df_enhanced['vague_question_error'].sum()\n",
    "non_vague_count = len(df_enhanced) - vague_count\n",
    "ax2.pie([non_vague_count, vague_count], \n",
    "        labels=['Specific Questions', 'Vague Questions'],\n",
    "        autopct='%1.1f%%', colors=['lightgreen', 'lightcoral'])\n",
    "ax2.set_title('Vague vs Specific Questions', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 3. Question type distribution for errors\n",
    "question_patterns = {\n",
    "    'Frequency': df_enhanced['question'].str.contains('how often|frequency', case=False, na=False).sum(),\n",
    "    'Dosage': df_enhanced['question'].str.contains('dose|dosage', case=False, na=False).sum(),\n",
    "    'Medication': df_enhanced['question'].str.contains('medication|drug', case=False, na=False).sum(),\n",
    "    'Temporal': df_enhanced['question'].str.contains('when|previous|current', case=False, na=False).sum(),\n",
    "}\n",
    "ax3.barh(list(question_patterns.keys()), list(question_patterns.values()), color='skyblue')\n",
    "ax3.set_title('Question Type Distribution in Errors', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Number of Examples')\n",
    "ax3.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 4. Error severity by medical pattern\n",
    "medical_pattern_cols = ['med_semantic_error', 'diagnostic_error', 'temporal_error', 'frequency_error', 'vague_question_error']\n",
    "severity_by_pattern = {}\n",
    "for col in medical_pattern_cols:\n",
    "    if df_enhanced[col].sum() > 0:\n",
    "        avg_severity = df_enhanced[df_enhanced[col]]['dataset_error_score'].mean()\n",
    "        severity_by_pattern[col.replace('_error', '').replace('_', ' ').title()] = avg_severity\n",
    "\n",
    "ax4.bar(range(len(severity_by_pattern)), list(severity_by_pattern.values()), color='orange')\n",
    "ax4.set_title('Average Error Severity by Medical Pattern', fontsize=14, fontweight='bold')\n",
    "ax4.set_ylabel('Average Error Score')\n",
    "ax4.set_xticks(range(len(severity_by_pattern)))\n",
    "ax4.set_xticklabels(list(severity_by_pattern.keys()), rotation=45, ha='right')\n",
    "ax4.axhline(y=3, color='red', linestyle='--', alpha=0.7, label='Error Threshold')\n",
    "ax4.legend()\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed medical pattern statistics\n",
    "print(\"=== MEDICAL DOMAIN PATTERN STATISTICS ===\")\n",
    "for col in medical_pattern_cols:\n",
    "    count = df_enhanced[col].sum()\n",
    "    percent = (count / len(df_enhanced)) * 100\n",
    "    if count > 0:\n",
    "        is_error_rate = df_enhanced[df_enhanced[col]]['is_dataset_error'].mean() * 100\n",
    "        print(f\"{col}: {count} ({percent:.1f}%) - Dataset Error Rate: {is_error_rate:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Complete Analysis Using Convenience Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the complete analysis pipeline for a comprehensive report\n",
    "results = analyze_medical_qa_errors(failed, error_threshold=2, length_percentile=95, verbose=True)\n",
    "\n",
    "# Access the enhanced dataframe for further analysis\n",
    "df_complete = results['enhanced_dataframe']\n",
    "\n",
    "# Show specific examples using the print function\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print_error_examples(df_complete, category='semantic_mismatch', max_examples=3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print_error_examples(df_complete, category='med_semantic_error', max_examples=3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print_error_examples(df_complete, category='vague_question_error', max_examples=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Unified Analysis of all the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Model Behavioral Pattern Analysis\n",
    "\n",
    "Analysis of specific model error behaviors (rules 12-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model behavior patterns\n",
    "model_behavior_summary = results['model_behavior_summary']\n",
    "print(\"Model Behavioral Patterns Summary:\")\n",
    "print(model_behavior_summary)\n",
    "print()\n",
    "\n",
    "# Count of examples with any model behavior error\n",
    "model_errors = df_enhanced['has_model_behavior_error'].sum()\n",
    "print(f\"\\nTotal examples with model behavior errors: {model_errors} ({model_errors/len(df_enhanced)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Visualize Model Behavior Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model behavior patterns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "model_cols = ['model_underextraction_pattern', 'model_entity_confusion_pattern',\n",
    "              'model_position_bias_pattern', 'model_boundary_failure_pattern']\n",
    "labels = ['Underextraction\\n(Model stopped early)', \n",
    "          'Entity Confusion\\n(Wrong entity, same type)',\n",
    "          'Position Bias\\n(Wrong location)', \n",
    "          'Boundary Failure\\n(Stopped at punctuation)']\n",
    "\n",
    "for idx, (col, label) in enumerate(zip(model_cols, labels)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Count distribution\n",
    "    counts = df_enhanced[col].value_counts()\n",
    "    ax.pie(counts.values, labels=['No Error', 'Has Error'], autopct='%1.1f%%',\n",
    "           colors=['#90EE90', '#FF6B6B'])\n",
    "    ax.set_title(label, fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_behavior_patterns.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total examples with any model behavior error: {df_enhanced['has_model_behavior_error'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Specific Examples of Model Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show specific examples for each model behavior pattern\n",
    "print(\"=\" * 100)\n",
    "print(\"SPECIFIC EXAMPLES OF MODEL BEHAVIORAL ERRORS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"\\n1. UNDEREXTRACTION (Model stopped too early):\")\n",
    "print(\"-\" * 100)\n",
    "underextract = df_enhanced[df_enhanced['model_underextraction_pattern']].head(3)\n",
    "for idx, row in underextract.iterrows():\n",
    "    print(f\"\\nQuestion: {row['question']}\")\n",
    "    print(f\"Gold:     {row['answer'][:100]}...\")\n",
    "    print(f\"Pred:     {row['predicted_answer'][:100]}...\")\n",
    "    print(f\"Error:    Model extracted only {len(row['predicted_answer'])} chars vs {len(row['answer'])} chars in gold\")\n",
    "\n",
    "print(\"\\n\\n2. ENTITY CONFUSION (Wrong entity, same semantic type):\")\n",
    "print(\"-\" * 100)\n",
    "entity_conf = df_enhanced[df_enhanced['model_entity_confusion_pattern']].head(3)\n",
    "for idx, row in entity_conf.iterrows():\n",
    "    print(f\"\\nQuestion: {row['question']}\")\n",
    "    print(f\"Gold:     {row['answer'][:100]}...\")\n",
    "    print(f\"Pred:     {row['predicted_answer'][:100]}...\")\n",
    "    print(f\"Error:    Model confused entities of same type (e.g., different medications)\")\n",
    "\n",
    "print(\"\\n\\n3. POSITION BIAS (Extracted from wrong location in context):\")\n",
    "print(\"-\" * 100)\n",
    "pos_bias = df_enhanced[df_enhanced['model_position_bias_pattern']].head(3)\n",
    "for idx, row in pos_bias.iterrows():\n",
    "    print(f\"\\nQuestion: {row['question']}\")\n",
    "    print(f\"Gold:     {row['answer'][:100]}...\")\n",
    "    print(f\"Pred:     {row['predicted_answer'][:100]}...\")\n",
    "    print(f\"Error:    Model selected answer from different position in document\")\n",
    "\n",
    "print(\"\\n\\n4. BOUNDARY FAILURE (Stopped at punctuation unnecessarily):\")\n",
    "print(\"-\" * 100)\n",
    "boundary = df_enhanced[df_enhanced['model_boundary_failure_pattern']].head(3)\n",
    "for idx, row in boundary.iterrows():\n",
    "    print(f\"\\nQuestion: {row['question']}\")\n",
    "    print(f\"Gold:     {row['answer'][:100]}...\")\n",
    "    print(f\"Pred:     {row['predicted_answer'][:100]}...\")\n",
    "    print(f\"Error:    Model stopped at punctuation but answer continues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Model vs Dataset Error Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model behavior errors vs dataset quality errors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Venn-like comparison\n",
    "categories = ['Model Behavior\\nErrors Only', 'Both Model &\\nDataset Issues', 'Dataset Quality\\nErrors Only']\n",
    "model_only = df_enhanced['has_model_behavior_error'] & ~df_enhanced['is_dataset_error']\n",
    "both = df_enhanced['has_model_behavior_error'] & df_enhanced['is_dataset_error']\n",
    "dataset_only = ~df_enhanced['has_model_behavior_error'] & df_enhanced['is_dataset_error']\n",
    "\n",
    "counts = [model_only.sum(), both.sum(), dataset_only.sum()]\n",
    "colors = ['#FF6B6B', '#FFD93D', '#6BCB77']\n",
    "\n",
    "ax[0].bar(categories, counts, color=colors)\n",
    "ax[0].set_ylabel('Number of Examples')\n",
    "ax[0].set_title('Model Behavior vs Dataset Quality Errors', fontweight='bold')\n",
    "ax[0].tick_params(axis='x', rotation=0)\n",
    "for i, v in enumerate(counts):\n",
    "    ax[0].text(i, v + 50, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Right: Breakdown percentages\n",
    "total = len(df_enhanced)\n",
    "percentages = [c/total*100 for c in counts]\n",
    "percentages.append((total - sum(counts))/total*100)  # Neither\n",
    "labels_pie = categories + ['Neither']\n",
    "colors_pie = colors + ['#E0E0E0']\n",
    "\n",
    "ax[1].pie(percentages, labels=labels_pie, autopct='%1.1f%%', colors=colors_pie, startangle=90)\n",
    "ax[1].set_title('Distribution of Error Types', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_vs_dataset_errors.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Model behavior errors only: {model_only.sum()} ({model_only.sum()/total*100:.1f}%)\")\n",
    "print(f\"Both model & dataset issues: {both.sum()} ({both.sum()/total*100:.1f}%)\")\n",
    "print(f\"Dataset quality errors only: {dataset_only.sum()} ({dataset_only.sum()/total*100:.1f}%)\")\n",
    "print(f\"Neither: {total - sum(counts)} ({(total - sum(counts))/total*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Advanced Visualizations for Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. SUNBURST CHART - Interactive Drill-down of Error Categories\n",
    "import plotly.express as px\n",
    "\n",
    "# Build sunburst data\n",
    "sunburst_data = []\n",
    "\n",
    "# Add root\n",
    "sunburst_data.append({\n",
    "    'labels': 'All Errors',\n",
    "    'parents': '',\n",
    "    'values': len(df_enhanced),\n",
    "    'text': f'{len(df_enhanced)} total'\n",
    "})\n",
    "\n",
    "# Level 1: Error vs Non-error\n",
    "has_error = df_enhanced['has_model_behavior_error'] | df_enhanced['is_dataset_error']\n",
    "sunburst_data.extend([\n",
    "    {'labels': 'Has Errors', 'parents': 'All Errors', \n",
    "     'values': has_error.sum(), 'text': f\"{has_error.sum()/len(df_enhanced)*100:.1f}%\"},\n",
    "    {'labels': 'No Errors', 'parents': 'All Errors', \n",
    "     'values': (~has_error).sum(), 'text': f\"{(~has_error).sum()/len(df_enhanced)*100:.1f}%\"}\n",
    "])\n",
    "\n",
    "# Level 2: Model vs Dataset\n",
    "model_only = df_enhanced['has_model_behavior_error'] & ~df_enhanced['is_dataset_error']\n",
    "dataset_only = ~df_enhanced['has_model_behavior_error'] & df_enhanced['is_dataset_error']\n",
    "both = df_enhanced['has_model_behavior_error'] & df_enhanced['is_dataset_error']\n",
    "\n",
    "sunburst_data.extend([\n",
    "    {'labels': 'Model Only', 'parents': 'Has Errors', \n",
    "     'values': model_only.sum(), 'text': f\"{model_only.sum()}\"},\n",
    "    {'labels': 'Dataset Only', 'parents': 'Has Errors', \n",
    "     'values': dataset_only.sum(), 'text': f\"{dataset_only.sum()}\"},\n",
    "    {'labels': 'Both', 'parents': 'Has Errors', \n",
    "     'values': both.sum(), 'text': f\"{both.sum()}\"}\n",
    "])\n",
    "\n",
    "# Level 3: Specific model patterns under \"Model Only\" and \"Both\"\n",
    "model_patterns = {\n",
    "    'Underextract': 'model_underextraction_pattern',\n",
    "    'Entity Confusion': 'model_entity_confusion_pattern',\n",
    "    'Position Bias': 'model_position_bias_pattern',\n",
    "    'Boundary Fail': 'model_boundary_failure_pattern'\n",
    "}\n",
    "\n",
    "for label, col in model_patterns.items():\n",
    "    # Under Model Only\n",
    "    count_model = (model_only & df_enhanced[col]).sum()\n",
    "    if count_model > 0:\n",
    "        sunburst_data.append({\n",
    "            'labels': f'{label} (M)', 'parents': 'Model Only',\n",
    "            'values': count_model, 'text': f\"{count_model}\"\n",
    "        })\n",
    "    \n",
    "    # Under Both\n",
    "    count_both = (both & df_enhanced[col]).sum()\n",
    "    if count_both > 0:\n",
    "        sunburst_data.append({\n",
    "            'labels': f'{label} (B)', 'parents': 'Both',\n",
    "            'values': count_both, 'text': f\"{count_both}\"\n",
    "        })\n",
    "\n",
    "sunburst_df = pd.DataFrame(sunburst_data)\n",
    "\n",
    "# Create sunburst\n",
    "fig = px.sunburst(\n",
    "    sunburst_df,\n",
    "    names='labels',\n",
    "    parents='parents',\n",
    "    values='values',\n",
    "    title='Interactive Error Category Breakdown (click to drill down)',\n",
    "    color='values',\n",
    "    color_continuous_scale='RdYlBu_r',\n",
    "    hover_data={'text': True}\n",
    ")\n",
    "\n",
    "fig.update_traces(textinfo='label+percent parent')\n",
    "fig.update_layout(height=700, font_size=11)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Analyze the Unified Analysis of everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "unified = pd.read_csv('../experiment_outputs/unified_analysis.csv')\n",
    "# pre-process to get the answer in a new col\n",
    "unified['answer'] = unified['answers'].apply(lambda x: ast.literal_eval(x)['text'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "The insights from cluster '-1' are noise and they don't have any particular pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "unified[unified['cluster'] == -1][['question', 'answer', 'rule3_low_q_similarity', 'rule5_qtype_mismatch', 'category']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "This gives an insight on all the hard examples that is very likely to be noise as they are labeled wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "unified[(unified['category'] == 'hard') & (unified['rule3_low_q_similarity'] == True) & (unified['cluster'] == 2)][['question', 'answer', 'rule3_low_q_similarity', 'cluster']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Cluster Level Analysis for the rules based errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all rule columns\n",
    "rule_cols = [col for col in unified.columns if col.startswith('rule')]\n",
    "\n",
    "# Count how many times each rule is triggered overall\n",
    "rule_summary = pd.DataFrame({\n",
    "    'count': unified[rule_cols].sum(),\n",
    "    'percent': unified[rule_cols].mean() * 100\n",
    "}).sort_values('count', ascending=False)\n",
    "\n",
    "print(f\"Total examples in unified dataset: {len(unified)}\")\n",
    "print(\"\\nOverall Rule trigger summary:\")\n",
    "print(rule_summary)\n",
    "\n",
    "# Per-cluster analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Rule trigger summary per cluster:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for cluster_id in sorted(unified['cluster'].unique()):\n",
    "    cluster_data = unified[unified['cluster'] == cluster_id]\n",
    "    \n",
    "    cluster_rule_summary = pd.DataFrame({\n",
    "        'count': cluster_data[rule_cols].sum(),\n",
    "        'percent': cluster_data[rule_cols].mean() * 100\n",
    "    }).sort_values('count', ascending=False)\n",
    "    \n",
    "    print(f\"\\n--- Cluster {cluster_id} (n={len(cluster_data)}) ---\")\n",
    "    print(cluster_rule_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Template Based Analysis of Train Dataset and Errors Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply rule-based analysis to failed examples for visualization\n",
    "failed_error_flags = failed.apply(compute_dataset_error_flags, 2, axis=1)\n",
    "failed_with_flags = pd.concat([failed, failed_error_flags], axis=1)\n",
    "\n",
    "# Compute summary for failed dataset\n",
    "failed_rule_cols = [c for c in failed_with_flags.columns if c.startswith(\"rule\")]\n",
    "failed_summary = pd.DataFrame({\n",
    "    \"count\": failed_with_flags[failed_rule_cols].sum(),\n",
    "    \"percent\": failed_with_flags[failed_rule_cols].mean() * 100\n",
    "}).sort_values(\"count\", ascending=False)\n",
    "\n",
    "print(f\"Total failed examples: {len(failed)}\")\n",
    "print(\"\\nRule trigger summary for failed predictions:\")\n",
    "print(failed_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Some additional analysis of these rules for ambiguous dataset from Dataset cartography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ambiguous dataset from cartography\n",
    "ambiguous_path = os.path.join(\"../cartography_analysis_big\", \"ambiguous_samples.json\")\n",
    "with open(ambiguous_path, \"r\") as f:\n",
    "    ambiguous_data = json.load(f)\n",
    "\n",
    "ambiguous_df = pd.DataFrame(ambiguous_data)\n",
    "\n",
    "ambiguous_df['answer'] = [v['text'][0] for v in ambiguous_df[\"answers\"].values]\n",
    "# Apply the same rule-based analysis to ambiguous examples\n",
    "ambiguous_error_flags = ambiguous_df.apply(compute_dataset_error_flags, axis=1)\n",
    "ambiguous_with_flags = pd.concat([ambiguous_df, ambiguous_error_flags], axis=1)\n",
    "\n",
    "# Compute summary for ambiguous dataset\n",
    "ambiguous_rule_cols = [c for c in ambiguous_with_flags.columns if c.startswith(\"rule\")]\n",
    "ambiguous_summary = pd.DataFrame({\n",
    "    \"count\": ambiguous_with_flags[ambiguous_rule_cols].sum(),\n",
    "    \"percent\": ambiguous_with_flags[ambiguous_rule_cols].mean() * 100\n",
    "}).sort_values(\"count\", ascending=False)\n",
    "\n",
    "print(f\"Total ambiguous examples: {len(ambiguous_df)}\")\n",
    "print(\"\\nRule trigger summary for ambiguous dataset:\")\n",
    "print(ambiguous_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "Visualizations for the Errors Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "summary[\"percent\"].plot(kind=\"bar\", color=\"blue\")\n",
    "plt.title(\"Dataset Error Rule Trigger Percentages\")\n",
    "plt.ylabel(\"Percentage (%)\")\n",
    "plt.xlabel(\"Rule\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = failed_with_flags[failed_with_flags[\"rule2_multi_clause\"]]\n",
    "r7 = failed_with_flags[failed_with_flags[\"rule7_boundary_weirdness\"]]\n",
    "counts = pd.Series({\n",
    "    \"Multi-Clause (Rule 2)\": r2.shape[0],\n",
    "    \"Boundary Weirdness (Rule 7)\": r7.shape[0]\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "counts.plot(kind=\"bar\", color=[\"green\", \"pink\"])\n",
    "plt.title(\"Major Dataset Artifact Types\")\n",
    "plt.ylabel(\"Number of Affected Examples\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis=\"y\", alpha=0.4, linestyle=\"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "###  Baseline EMRQA prediction error analysis\n",
    "- High-level error patterns for `eval_baseline_emrqa/eval_predictions.jsonl`\n",
    "- Label wrong predictions as `truncated_span`, `overlong_span`, `partial_overlap`, or `no_overlap` and show counts/examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "token_re = re.compile(r\"\\w+\")\n",
    "relation = [\"truncated_span\", \"overrun_span\", \"partial_overlap\", \"no_overlap\"]\n",
    "\n",
    "def norm(text):\n",
    "    return \" \".join(text.lower().split())\n",
    "\n",
    "def tokens(text: str):\n",
    "    return token_re.findall(text.lower())\n",
    "\n",
    "def span_relation(prediction: str, actual: str):\n",
    "    p = norm(prediction)\n",
    "    a = norm(actual)\n",
    "    if p and p in a:\n",
    "        return relation[0]\n",
    "    if a and a in p:\n",
    "        return relation[1]\n",
    "    if set(tokens(p)) & set(tokens(a)):\n",
    "        return relation[2]\n",
    "    return relation[3]\n",
    "\n",
    "# Add a new column to the failed dataframe for the span relation\n",
    "failed['relation'] = failed.apply(\n",
    "    lambda row: span_relation(\n",
    "        row.get(\"predicted_answer\", \"\"),\n",
    "        row.get(\"answer\", \"\")\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Display counts from the failed dataframe\n",
    "print(f\"Total wrong predictions: {len(failed)}\")\n",
    "print(\"Wrong predictions by type:\")\n",
    "print(failed[\"relation\"].value_counts())\n",
    "\n",
    "# Use the following filter to deep dive\n",
    "# Sample examples from each relation type\n",
    "samples = []\n",
    "for rel in relation:\n",
    "    rel_samples = failed[failed[\"relation\"] == rel][[\"id\", \"question\", \"predicted_answer\", \"answer\", \"relation\"]].head(3)\n",
    "    samples.append(rel_samples)\n",
    "\n",
    "pd.concat(samples, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### Deep Dive into Baseline EMRQA Prediction Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "Cluster error types in failed examples for further analysis. This uses sentence transformers to embed the question, context, and predicted/true answers, then clusters them using KMeans. This seems to work better. However, the clusters are still not very meaningful. Further work is needed here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### Manual Review of Sampled Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "- No. of questions in the train dataset that start with \"has\" is 62872/130,956\n",
    "- No. of questions in the validation dataset that start with \"has\" is 15913/32739\n",
    "- No. of questions in the failed examples that start with \"has\" is 1423/3248"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "Insulin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "insulin_questions = failed[failed['question'].str.contains('insulin', case=False, na=False)]\n",
    "len(insulin_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "Chest pain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "chest_pain_questions = failed[failed['question'].str.contains('chest pain', case=False, na=False)]\n",
    "len(chest_pain_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "Medication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "medication_questions = failed[failed['question'].str.contains('medication', case=False, na=False)]\n",
    "len(chest_pain_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "Gemfibrozil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemfibrozil_examples = failed[failed['question'].str.lower().str.contains('gemfibrozil')]\n",
    "len(gemfibrozil_examples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
