{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import textwrap\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pretty-print JSON with wrapped lines\n",
    "def pretty_print_json(data, max_width=80):\n",
    "    json_str = json.dumps(data, indent=4)\n",
    "    wrapped_lines = []\n",
    "    for line in json_str.splitlines():\n",
    "        wrapped_lines.extend(textwrap.wrap(line, width=max_width))\n",
    "    return \"\\n\".join(wrapped_lines)\n",
    "\n",
    "file_path = os.path.join(\"eval_baseline_emrqa\", \"eval_predictions.jsonl\")\n",
    "with open(file_path, \"r\") as file:\n",
    "    predictions = [json.loads(line) for line in file]\n",
    "\n",
    "# Filter failed examples where the predicted answer does not match the expected answer\n",
    "failed_examples = [\n",
    "    prediction for prediction in predictions\n",
    "    if prediction['predicted_answer'] != prediction['answers']['text'][0] # for whatever reason this is a list\n",
    "]\n",
    "\n",
    "# Put Failed examples in a dataframe\n",
    "failed = pd.DataFrame(failed_examples)\n",
    "failed['answer'] = [v['text'][0] for v in failed[\"answers\"].values]\n",
    "# pd.set_option('display.max_colwidth', 20)\n",
    "failed.columns\n",
    "len(failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# failed[failed['question'].str.lower().str.startswith('has')].head(50)\n",
    "gemfibrozil_examples = failed[failed['question'].str.lower().str.contains('gemfibrozil')]\n",
    "gemfibrozil_examples.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(failed[failed['predicted_answer'] == ',']), len(failed[failed['predicted_answer'] == '.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "###  Baseline EMRQA prediction error analysis\n",
    "- High-level error patterns for `eval_baseline_emrqa/eval_predictions.jsonl`\n",
    "- Label wrong predictions as `truncated_span`, `overlong_span`, `partial_overlap`, or `no_overlap` and show counts/examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "token_re = re.compile(r\"\\w+\")\n",
    "relation = [\"truncated_span\", \"overrun_span\", \"partial_overlap\", \"no_overlap\"]\n",
    "\n",
    "def norm(text):\n",
    "    return \" \".join(text.lower().split())\n",
    "\n",
    "def tokens(text: str):\n",
    "    return token_re.findall(text.lower())\n",
    "\n",
    "def span_relation(prediction: str, actual: str):\n",
    "    p = norm(prediction)\n",
    "    a = norm(actual)\n",
    "    if p and p in a:\n",
    "        return relation[0]\n",
    "    if a and a in p:\n",
    "        return relation[1]\n",
    "    if set(tokens(p)) & set(tokens(a)):\n",
    "        return relation[2]\n",
    "    return relation[3]\n",
    "\n",
    "# Add a new column to the failed dataframe for the span relation\n",
    "failed['relation'] = failed.apply(\n",
    "    lambda row: span_relation(\n",
    "        row.get(\"predicted_answer\", \"\"),\n",
    "        row.get(\"answer\", \"\")\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Display counts from the failed dataframe\n",
    "print(f\"Total wrong predictions: {len(failed)}\")\n",
    "print(\"Wrong predictions by type:\")\n",
    "print(failed[\"relation\"].value_counts())\n",
    "\n",
    "# Use the following filter to deep dive\n",
    "# Sample examples from each relation type\n",
    "samples = []\n",
    "for rel in relation:\n",
    "    rel_samples = failed[failed[\"relation\"] == rel][[\"id\", \"question\", \"predicted_answer\", \"answer\", \"relation\"]].head(3)\n",
    "    samples.append(rel_samples)\n",
    "\n",
    "pd.concat(samples, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Deep Dive into Baseline EMRQA Prediction Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed['gold_norm'] = failed['answer'].apply(norm)\n",
    "failed['pred_norm'] = failed['predicted_answer'].apply(norm)\n",
    "failed[\"clust_text\"] = failed.apply(\n",
    "    lambda r: f\"Q: {r['question']} GOLD: {r['gold_norm']} PRED: {r['pred_norm']}\",\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Tfidf + k-means did not yield meaningful clusters here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=3000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(failed[\"clust_text\"])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "kmeans = KMeans(n_clusters=k, random_state=0, n_init=10, max_iter=100)\n",
    "cluster_labels = kmeans.fit_predict(X)\n",
    "\n",
    "failed[\"cluster\"] = cluster_labels\n",
    "cluster_sizes = failed[\"cluster\"].value_counts().sort_index()\n",
    "cluster_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Per Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_summaries = {}\n",
    "\n",
    "for c in sorted(failed[\"cluster\"].unique()):\n",
    "    sub = failed[failed[\"cluster\"] == c]\n",
    "\n",
    "    qs = sub[\"question\"].astype(str).tolist()\n",
    "    if len(qs) < 5:\n",
    "        continue\n",
    "\n",
    "    cv = CountVectorizer(\n",
    "        max_features=2000,\n",
    "        ngram_range=(1,3),\n",
    "        stop_words=\"english\"\n",
    "    )\n",
    "    Xq = cv.fit_transform(qs)\n",
    "\n",
    "    sums = np.asarray(Xq.sum(axis=0)).ravel()\n",
    "    terms = np.array(cv.get_feature_names_out())\n",
    "    top_idx = sums.argsort()[::-1][:15]\n",
    "\n",
    "    cluster_summaries[c] = {\n",
    "        \"n\": len(sub),\n",
    "        \"top_terms\": list(terms[top_idx]),\n",
    "        \"error_dist\": sub[\"relation\"].value_counts().to_dict()\n",
    "    }\n",
    "\n",
    "cluster_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in sorted(cluster_summaries.keys()):\n",
    "    info = cluster_summaries[c]\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Cluster {c}  |  n = {info['n']}\")\n",
    "    print(\"Top terms:\", \", \".join(info[\"top_terms\"]))\n",
    "    print(\"Error types:\", info[\"error_dist\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = list(range(2, 25))\n",
    "sse = []\n",
    "\n",
    "for k in ks:\n",
    "    km = KMeans(n_clusters=k, random_state=0, n_init=10, max_iter=200)\n",
    "    km.fit(X)\n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "plt.plot(ks, sse, marker='o')\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Inertia (SSE)\")\n",
    "plt.title(\"Elbow Method\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Cluster error types in failed examples for further analysis. This uses sentence transformers to embed the question, context, and predicted/true answers, then clusters them using KMeans. This seems to work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "\n",
    "embed_texts = failed[\"clust_text\"].tolist()\n",
    "embeddings = model.encode(\n",
    "    embed_texts,\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pca_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "failed[\"pca_x\"] = pca_2d[:, 0]\n",
    "failed[\"pca_y\"] = pca_2d[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "ks = list(range(2, 16))\n",
    "sil_scores = []\n",
    "\n",
    "for k in ks:\n",
    "    km = KMeans(\n",
    "        n_clusters=k,\n",
    "        random_state=0,\n",
    "        n_init=10,\n",
    "        max_iter=300\n",
    "    )\n",
    "    labels = km.fit_predict(embeddings)\n",
    "    score = silhouette_score(embeddings, labels)\n",
    "    sil_scores.append(score)\n",
    "    print(f\"k={k:2d}  silhouette={score:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(ks, sil_scores, marker=\"o\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Silhouette score\")\n",
    "plt.title(\"Silhouette scores (KMeans on embeddings)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "best_k = 8\n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=best_k,\n",
    "    random_state=0,\n",
    "    n_init=10,\n",
    "    max_iter=300\n",
    ")\n",
    "k_labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "failed[\"cluster_kmeans\"] = k_labels\n",
    "failed[\"cluster_kmeans\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for cid in sorted(failed[\"cluster_kmeans\"].unique()):\n",
    "    mask = failed[\"cluster_kmeans\"] == cid\n",
    "    plt.scatter(\n",
    "        failed.loc[mask, \"pca_x\"],\n",
    "        failed.loc[mask, \"pca_y\"],\n",
    "        s=5,\n",
    "        alpha=0.7,\n",
    "        label=f\"cluster {cid}\"\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"PCA-1\")\n",
    "plt.ylabel(\"PCA-2\")\n",
    "plt.title(f\"KMeans (k={best_k}) on embeddings (PCA 2D)\")\n",
    "plt.legend(markerscale=3, bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "cluster_k_info = {}\n",
    "\n",
    "for cid in sorted(failed[\"cluster_kmeans\"].unique()):\n",
    "    sub = failed[failed[\"cluster_kmeans\"] == cid]\n",
    "    n = len(sub)\n",
    "    rel_dist = sub[\"relation\"].value_counts().to_dict()\n",
    "    \n",
    "    top_terms = []\n",
    "    if n >= 5:\n",
    "        cv = CountVectorizer(\n",
    "            max_features=2000,\n",
    "            ngram_range=(1,2),\n",
    "            stop_words=\"english\"\n",
    "        )\n",
    "        Xq = cv.fit_transform(sub[\"question\"].astype(str).tolist())\n",
    "        sums = np.asarray(Xq.sum(axis=0)).ravel()\n",
    "        terms = np.array(cv.get_feature_names_out())\n",
    "        top_idx = sums.argsort()[::-1][:15]\n",
    "        top_terms = list(terms[top_idx])\n",
    "    \n",
    "    cluster_k_info[cid] = {\n",
    "        \"n\": n,\n",
    "        \"relation_dist\": rel_dist,\n",
    "        \"top_terms\": top_terms,\n",
    "    }\n",
    "\n",
    "for cid, info in cluster_k_info.items():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Cluster {cid}  |  n = {info['n']}\")\n",
    "    print(\"Error-type distribution:\", info[\"relation_dist\"])\n",
    "    print(\"Top question terms:\", \", \".join(info[\"top_terms\"]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Manual Review of Sampled Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "- No. of questions in the train dataset that start with \"has\" is 62872/130,956\n",
    "- No. of questions in the validation dataset that start with \"has\" is 15913/32739\n",
    "- No. of questions in the failed examples that start with \"has\" is 1423/3248"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "This examines the whole dataset for train and eval to see how many questions start with \"has\", 'why' etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "train = load_dataset(\"Eladio/emrqa-msquad\", split=\"train\")\n",
    "\n",
    "# Filter questions that start with \"why\"\n",
    "why_questions = train.filter(lambda example: example[\"question\"].lower().startswith(\"why\"))\n",
    "\n",
    "why_questions_df = pd.DataFrame(why_questions)\n",
    "len(why_questions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "eval = load_dataset(\"Eladio/emrqa-msquad\", split=\"validation\")\n",
    "\n",
    "# Filter questions that start with \"has\"\n",
    "has_questions = eval.filter(lambda example: example[\"question\"].lower().startswith(\"has\"))\n",
    "\n",
    "# Display the filtered examples\n",
    "has_questions_df = pd.DataFrame(has_questions)\n",
    "has_questions_df['question']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Insulin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "insulin_questions = failed[failed['question'].str.contains('insulin', case=False, na=False)]\n",
    "len(insulin_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Chest pain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "chest_pain_questions = failed[failed['question'].str.contains('chest pain', case=False, na=False)]\n",
    "len(chest_pain_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "Medication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "medication_questions = failed[failed['question'].str.contains('medication', case=False, na=False)]\n",
    "len(chest_pain_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "# Generating the sample data from make_blobs\n",
    "# This particular setting has one distinct cluster and 3 clusters placed close\n",
    "# together.\n",
    "X, y = make_blobs(\n",
    "    n_samples=500,\n",
    "    n_features=2,\n",
    "    centers=4,\n",
    "    cluster_std=1,\n",
    "    center_box=(-10.0, 10.0),\n",
    "    shuffle=True,\n",
    "    random_state=1,\n",
    ")  # For reproducibility\n",
    "\n",
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(\n",
    "        X[:, 0], X[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(\n",
    "        centers[:, 0],\n",
    "        centers[:, 1],\n",
    "        marker=\"o\",\n",
    "        c=\"white\",\n",
    "        alpha=1,\n",
    "        s=200,\n",
    "        edgecolor=\"k\",\n",
    "    )\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n",
    "        % n_clusters,\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final-project (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
