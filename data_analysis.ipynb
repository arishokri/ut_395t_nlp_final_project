{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import textwrap\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pretty-print JSON with wrapped lines\n",
    "def pretty_print_json(data, max_width=80):\n",
    "    json_str = json.dumps(data, indent=4)\n",
    "    wrapped_lines = []\n",
    "    for line in json_str.splitlines():\n",
    "        wrapped_lines.extend(textwrap.wrap(line, width=max_width))\n",
    "    return \"\\n\".join(wrapped_lines)\n",
    "\n",
    "file_path = os.path.join(\"eval_baseline_emrqa\", \"eval_predictions.jsonl\")\n",
    "with open(file_path, \"r\") as file:\n",
    "    predictions = [json.loads(line) for line in file]\n",
    "\n",
    "# Filter failed examples where the predicted answer does not match the expected answer\n",
    "failed_examples = [\n",
    "    prediction for prediction in predictions\n",
    "    if prediction['predicted_answer'] != prediction['answers']['text'][0] # for whatever reason this is a list\n",
    "]\n",
    "\n",
    "# Put Failed examples in a dataframe\n",
    "failed = pd.DataFrame(failed_examples)\n",
    "failed['answer'] = [v['text'][0] for v in failed[\"answers\"].values]\n",
    "# pd.set_option('display.max_colwidth', 20)\n",
    "failed.columns\n",
    "len(failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1eb6e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 217 (3037764688.py, line 233)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 233\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef detect_dataset_error(example, stats, min_rules_triggered=2):\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after function definition on line 217\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# Basic text utilities\n",
    "############################\n",
    "\n",
    "STOPWORDS = {\n",
    "    \"the\", \"a\", \"an\", \"of\", \"to\", \"and\", \"or\", \"in\", \"on\", \"for\",\n",
    "    \"with\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\n",
    "    \"at\", \"by\", \"from\", \"as\", \"that\", \"this\", \"these\", \"those\",\n",
    "    \"it\", \"its\", \"he\", \"she\", \"they\", \"them\", \"his\", \"her\", \"their\",\n",
    "    \"we\", \"you\", \"i\", \"but\", \"if\", \"then\", \"so\", \"because\", \"when\",\n",
    "    \"while\", \"about\", \"into\", \"out\", \"up\", \"down\", \"over\", \"under\"\n",
    "}\n",
    "\n",
    "WORD_RE = re.compile(r\"\\w+\")\n",
    "DATE_RE = re.compile(\n",
    "    r\"\\b(?:\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|\\d{4}|jan(?:uary)?|feb(?:ruary)?|mar(?:ch)?|apr(?:il)?|\"\n",
    "    r\"may|jun(?:e)?|jul(?:y)?|aug(?:ust)?|sep(?:t(?:ember)?)?|oct(?:ober)?|nov(?:ember)?|dec(?:ember)?)\\b\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "NUMBER_RE = re.compile(r\"\\b\\d+(?:\\.\\d+)?\\b\")\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    return \" \".join(text.lower().split())\n",
    "\n",
    "def tokenize(text: str):\n",
    "    return WORD_RE.findall(normalize(text))\n",
    "\n",
    "def jaccard_similarity(a_tokens, b_tokens):\n",
    "    a_set, b_set = set(a_tokens), set(b_tokens)\n",
    "    if not a_set or not b_set:\n",
    "        return 0.0\n",
    "    inter = len(a_set & b_set)\n",
    "    union = len(a_set | b_set)\n",
    "    return inter / union\n",
    "\n",
    "# to identify repetitive/uninformative answers\n",
    "def shannon_entropy(tokens):\n",
    "    if not tokens:\n",
    "        return 0.0\n",
    "    counts = Counter(tokens)\n",
    "    total = sum(counts.values())\n",
    "    ent = 0.0\n",
    "    for c in counts.values():\n",
    "        p = c / total\n",
    "        ent -= p * math.log2(p)\n",
    "    return ent\n",
    "\n",
    "def count_dates(text: str) -> int:\n",
    "    return len(DATE_RE.findall(text))\n",
    "\n",
    "def count_numbers(text: str) -> int:\n",
    "    return len(NUMBER_RE.findall(text))\n",
    "\n",
    "def stopword_ratio(tokens):\n",
    "    if not tokens:\n",
    "        return 1.0\n",
    "    sw = sum(1 for t in tokens if t in STOPWORDS)\n",
    "    return sw / len(tokens)\n",
    "\n",
    "\n",
    "############################\n",
    "# Dataset-level stats\n",
    "############################\n",
    "\n",
    "def compute_dataset_stats(examples):\n",
    "    \"\"\"\n",
    "    Compute basic stats over gold spans & question-span similarity\n",
    "    so rules can use dynamic thresholds.\n",
    "    \"\"\"\n",
    "    gold_lengths = []\n",
    "    qg_sims = []\n",
    "\n",
    "    for ex in examples:\n",
    "        gold = ex[\"answers\"][\"text\"][0]\n",
    "        question = ex[\"question\"]\n",
    "        gold_tokens = tokenize(gold)\n",
    "        q_tokens = tokenize(question)\n",
    "\n",
    "        if not gold_tokens:\n",
    "            continue\n",
    "\n",
    "        gold_lengths.append(len(gold_tokens))\n",
    "        qg_sims.append(jaccard_similarity(q_tokens, gold_tokens))\n",
    "\n",
    "    if not gold_lengths:\n",
    "        # degenerate case\n",
    "        return {\n",
    "            \"median_gold_len\": 0,\n",
    "            \"mad_gold_len\": 0,\n",
    "            \"p95_gold_len\": 0,\n",
    "            \"mean_qg_sim\": 0.0,\n",
    "            \"std_qg_sim\": 0.0,\n",
    "        }\n",
    "\n",
    "    # median and MAD\n",
    "    med_len = median(gold_lengths)\n",
    "    abs_dev = [abs(l - med_len) for l in gold_lengths]\n",
    "    mad_len = median(abs_dev)\n",
    "\n",
    "    # rough p95\n",
    "    sorted_lens = sorted(gold_lengths)\n",
    "    idx_95 = int(0.95 * (len(sorted_lens) - 1))\n",
    "    p95_len = sorted_lens[idx_95]\n",
    "\n",
    "    # mean and std of q–gold similarity\n",
    "    if qg_sims:\n",
    "        mean_sim = sum(qg_sims) / len(qg_sims)\n",
    "        var_sim = sum((s - mean_sim) ** 2 for s in qg_sims) / max(len(qg_sims) - 1, 1)\n",
    "        std_sim = math.sqrt(var_sim)\n",
    "    else:\n",
    "        mean_sim, std_sim = 0.0, 0.0\n",
    "\n",
    "    return {\n",
    "        \"median_gold_len\": med_len,\n",
    "        \"mad_gold_len\": mad_len,\n",
    "        \"p95_gold_len\": p95_len,\n",
    "        \"mean_qg_sim\": mean_sim,\n",
    "        \"std_qg_sim\": std_sim,\n",
    "    }\n",
    "\n",
    "\n",
    "############################\n",
    "# Rule implementations\n",
    "############################\n",
    "\n",
    "# Extremely long answers may indicate annotation errors where too much text was selected\n",
    "def rule1_length_anomaly(gold_tokens, stats):\n",
    "    L = len(gold_tokens)\n",
    "    med = stats[\"median_gold_len\"]\n",
    "    mad = stats[\"mad_gold_len\"]\n",
    "    p95 = stats[\"p95_gold_len\"]\n",
    "\n",
    "    too_long_vs_p95 = L > p95\n",
    "    too_long_vs_mad = mad > 0 and L > med + 2 * mad\n",
    "    return too_long_vs_p95 or too_long_vs_mad\n",
    "\n",
    "def split_clauses(text: str):\n",
    "    # very rough clause segmentation\n",
    "    clauses = re.split(r'[,\\n;]| and | or ', text)\n",
    "    # keep clauses that have at least 2 non-stopword tokens\n",
    "    clean_clauses = []\n",
    "    for c in clauses:\n",
    "        toks = tokenize(c)\n",
    "        non_sw = [t for t in toks if t not in STOPWORDS]\n",
    "        if len(non_sw) >= 2:\n",
    "            clean_clauses.append(c.strip())\n",
    "    return clean_clauses\n",
    "\n",
    "# Multi-clause answers may be over segmented or contain irrelevant information\n",
    "def rule2_multi_clause(gold_text: str):\n",
    "    clauses = split_clauses(gold_text)\n",
    "    return len(clauses) > 1\n",
    "\n",
    "\n",
    "# def rule3_low_question_similarity(question_tokens, gold_tokens, stats, k=1.0):\n",
    "#     sim = jaccard_similarity(question_tokens, gold_tokens)\n",
    "#     mean_sim = stats[\"mean_qg_sim\"]\n",
    "#     std_sim = stats[\"std_qg_sim\"]\n",
    "#     # flag if sim is much lower than typical q–gold sim\n",
    "#     threshold = mean_sim - k * std_sim\n",
    "#     # if std_sim == 0, threshold == mean_sim; low sim will be 0, so flagged\n",
    "#     return sim < threshold\n",
    "\n",
    "# Most commonly observed issue: The Model extracts a more relevant portion of an over-annotated gold answer\n",
    "def rule4_pred_inside_gold_and_better_align(pred_text, gold_text, question_tokens):\n",
    "    gold_norm = normalize(gold_text)\n",
    "    pred_norm = normalize(pred_text)\n",
    "    # substring check in normalized space\n",
    "    if pred_norm and pred_norm in gold_norm:\n",
    "        gold_tokens = tokenize(gold_text)\n",
    "        pred_tokens = tokenize(pred_text)\n",
    "        sim_gold = jaccard_similarity(question_tokens, gold_tokens)\n",
    "        sim_pred = jaccard_similarity(question_tokens, pred_tokens)\n",
    "        return sim_pred > sim_gold + 0.1  # margin; tune if needed\n",
    "    return False\n",
    "\n",
    "def classify_question_type(question: str):\n",
    "    q = question.lower()\n",
    "    if any(x in q for x in [\"when\", \"date\", \"time\", \"year\", \"day\", \"month\"]):\n",
    "        return \"time\"\n",
    "    if \"how many\" in q or \"number of\" in q or \"count\" in q:\n",
    "        return \"number\"\n",
    "    # you can expand this as needed (medication, lab, diagnosis, etc.)\n",
    "    return \"other\"\n",
    "\n",
    "def rule5_question_type_mismatch(gold_text: str, question: str):\n",
    "    q_type = classify_question_type(question)\n",
    "    if q_type == \"time\":\n",
    "        n_dates = count_dates(gold_text)\n",
    "        return n_dates != 1  # we expect exactly one\n",
    "    if q_type == \"number\":\n",
    "        n_nums = count_numbers(gold_text)\n",
    "        return n_nums != 1\n",
    "    return False  # for \"other\" we don't use this rule\n",
    "\n",
    "def rule6_multiple_occurrences(gold_text: str, context: str):\n",
    "    gold_norm = normalize(gold_text)\n",
    "    ctx_norm = normalize(context)\n",
    "    if not gold_norm:\n",
    "        return False\n",
    "    return ctx_norm.count(gold_norm) > 1\n",
    "\n",
    "# lot of answers only have '.' or ',' so it must be predicting wrong span.\n",
    "def rule7_boundary_weirdness(gold_text: str):\n",
    "    stripped = gold_text.strip()\n",
    "    if not stripped:\n",
    "        return False\n",
    "    starts_bad = stripped[0] in {\",\", \";\", \":\", \"-\", \".\"}\n",
    "    ends_bad = stripped[-1] in {\",\", \";\", \":\", \"-\", \"(\", \"/\"}\n",
    "    return starts_bad or ends_bad\n",
    "\n",
    "def rule8_low_entropy(gold_tokens):\n",
    "    ent = shannon_entropy(gold_tokens)\n",
    "    # heuristic: very low entropy for longer spans is suspicious\n",
    "    return len(gold_tokens) >= 5 and ent < 1.0\n",
    "\n",
    "# def rule9_high_stopword_ratio(gold_tokens, threshold=0.7):\n",
    "#     return stopword_ratio(gold_tokens) > threshold\n",
    "\n",
    "# # prediction better than answers\n",
    "# def rule10_pred_better_answers_question(pred_text, gold_text, question_tokens):\n",
    "#     gold_tokens = tokenize(gold_text)\n",
    "#     pred_tokens = tokenize(pred_text)\n",
    "#     sim_gold = jaccard_similarity(question_tokens, gold_tokens)\n",
    "#     sim_pred = jaccard_similarity(question_tokens, pred_tokens)\n",
    "#     return sim_pred > sim_gold + 0.1  # margin\n",
    "\n",
    "\n",
    "############################\n",
    "# Overall dataset-error detector\n",
    "############################\n",
    "\n",
    "def detect_dataset_error(example, stats, min_rules_triggered=2):\n",
    "    \"\"\"\n",
    "    Returns (is_dataset_error, details_dict)\n",
    "    details_dict tells you which rules fired.\n",
    "    \"\"\"\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    gold_text = example[\"answers\"][\"text\"][0]\n",
    "    pred_text = example.get(\"predicted_answer\", \"\")\n",
    "\n",
    "    gold_tokens = tokenize(gold_text)\n",
    "    question_tokens = tokenize(question)\n",
    "\n",
    "    # individual rule flags\n",
    "    flags = {}\n",
    "\n",
    "    flags[\"rule1_length_anomaly\"] = rule1_length_anomaly(gold_tokens, stats)\n",
    "    flags[\"rule2_multi_clause\"] = rule2_multi_clause(gold_text)\n",
    "    #flags[\"rule3_low_q_sim\"] = rule3_low_question_similarity(question_tokens, gold_tokens, stats)\n",
    "    flags[\"rule4_pred_inside_gold_better_align\"] = rule4_pred_inside_gold_and_better_align(\n",
    "        pred_text, gold_text, question_tokens\n",
    "    )\n",
    "    flags[\"rule5_qtype_mismatch\"] = rule5_question_type_mismatch(gold_text, question)\n",
    "    flags[\"rule6_multi_occurrences\"] = rule6_multiple_occurrences(gold_text, context)\n",
    "    flags[\"rule7_boundary_weirdness\"] = rule7_boundary_weirdness(gold_text)\n",
    "    flags[\"rule8_low_entropy\"] = rule8_low_entropy(gold_tokens)\n",
    "    # flags[\"rule9_high_stopword_ratio\"] = rule9_high_stopword_ratio(gold_tokens)\n",
    "    # flags[\"rule10_pred_better_answer\"] = rule10_pred_better_answers_question(pred_text, gold_text, question_tokens)\n",
    "\n",
    "    num_true = sum(flags.values())\n",
    "    is_error = num_true >= min_rules_triggered\n",
    "\n",
    "    return is_error, flags\n",
    "\n",
    "\n",
    "stats = compute_dataset_stats(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88ae9189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples: 32739\n",
      "Dataset errors: 11249\n",
      "Percentage: 34.35963224289074 %\n",
      "Model failures: 3248\n",
      "Dataset errors among failures: 2829\n",
      "Percentage: 87.09975369458128 %\n",
      "=== Rule Trigger Rates ===\n",
      "rule1_length_anomaly: 4176 (12.76%)\n",
      "rule2_multi_clause: 10250 (31.31%)\n",
      "rule3_low_q_sim: 3002 (9.17%)\n",
      "rule4_pred_inside_gold_better_align: 19 (0.06%)\n",
      "rule5_qtype_mismatch: 722 (2.21%)\n",
      "rule6_multi_occurrences: 1871 (5.71%)\n",
      "rule7_boundary_weirdness: 21825 (66.66%)\n",
      "rule8_low_entropy: 0 (0.00%)\n",
      "rule9_high_stopword_ratio: 0 (0.00%)\n",
      "rule10_pred_better_answer: 57 (0.17%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = []\n",
    "for ex in predictions:\n",
    "    is_err, flags = detect_dataset_error(ex, stats, min_rules_triggered=2)\n",
    "    results.append({\n",
    "        \"id\": ex[\"id\"],\n",
    "        \"is_dataset_error\": is_err,\n",
    "        \"flags\": flags,\n",
    "        \"gold\": ex[\"answers\"][\"text\"][0],\n",
    "        \"pred\": ex.get(\"predicted_answer\", \"\"),\n",
    "        \"question\": ex[\"question\"],\n",
    "    })\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "total = len(df)\n",
    "num_ds_errors = df[\"is_dataset_error\"].sum()\n",
    "\n",
    "print(\"Total examples:\", total)\n",
    "print(\"Dataset errors:\", num_ds_errors)\n",
    "print(\"Percentage:\", num_ds_errors / total * 100, \"%\")\n",
    "\n",
    "def is_failure(row):\n",
    "    return row[\"pred\"] != row[\"gold\"]\n",
    "\n",
    "df[\"is_failure\"] = df.apply(is_failure, axis=1)\n",
    "\n",
    "failed_df = df[df[\"is_failure\"] == True]\n",
    "\n",
    "fail_total = len(failed_df)\n",
    "fail_ds_errors = failed_df[\"is_dataset_error\"].sum()\n",
    "\n",
    "print(\"Model failures:\", fail_total)\n",
    "print(\"Dataset errors among failures:\", fail_ds_errors)\n",
    "print(\"Percentage:\", fail_ds_errors / fail_total * 100, \"%\")\n",
    "rule_columns = list(results[0][\"flags\"].keys())\n",
    "\n",
    "rule_counts = {rule: 0 for rule in rule_columns}\n",
    "\n",
    "for r in results:\n",
    "    for rule, val in r[\"flags\"].items():\n",
    "        if val:\n",
    "            rule_counts[rule] += 1\n",
    "\n",
    "print(\"=== Rule Trigger Rates ===\")\n",
    "for rule, count in rule_counts.items():\n",
    "    print(f\"{rule}: {count} ({count/total*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "368f34dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# failed[failed['question'].str.lower().str.startswith('has')].head(50)\n",
    "gemfibrozil_examples = failed[failed['question'].str.lower().str.contains('gemfibrozil')]\n",
    "gemfibrozil_examples.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(failed[failed['predicted_answer'] == ',']), len(failed[failed['predicted_answer'] == '.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "###  Baseline EMRQA prediction error analysis\n",
    "- High-level error patterns for `eval_baseline_emrqa/eval_predictions.jsonl`\n",
    "- Label wrong predictions as `truncated_span`, `overlong_span`, `partial_overlap`, or `no_overlap` and show counts/examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "token_re = re.compile(r\"\\w+\")\n",
    "relation = [\"truncated_span\", \"overrun_span\", \"partial_overlap\", \"no_overlap\"]\n",
    "\n",
    "def norm(text):\n",
    "    return \" \".join(text.lower().split())\n",
    "\n",
    "def tokens(text: str):\n",
    "    return token_re.findall(text.lower())\n",
    "\n",
    "def span_relation(prediction: str, actual: str):\n",
    "    p = norm(prediction)\n",
    "    a = norm(actual)\n",
    "    if p and p in a:\n",
    "        return relation[0]\n",
    "    if a and a in p:\n",
    "        return relation[1]\n",
    "    if set(tokens(p)) & set(tokens(a)):\n",
    "        return relation[2]\n",
    "    return relation[3]\n",
    "\n",
    "# Add a new column to the failed dataframe for the span relation\n",
    "failed['relation'] = failed.apply(\n",
    "    lambda row: span_relation(\n",
    "        row.get(\"predicted_answer\", \"\"),\n",
    "        row.get(\"answer\", \"\")\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Display counts from the failed dataframe\n",
    "print(f\"Total wrong predictions: {len(failed)}\")\n",
    "print(\"Wrong predictions by type:\")\n",
    "print(failed[\"relation\"].value_counts())\n",
    "\n",
    "# Use the following filter to deep dive\n",
    "# Sample examples from each relation type\n",
    "samples = []\n",
    "for rel in relation:\n",
    "    rel_samples = failed[failed[\"relation\"] == rel][[\"id\", \"question\", \"predicted_answer\", \"answer\", \"relation\"]].head(3)\n",
    "    samples.append(rel_samples)\n",
    "\n",
    "pd.concat(samples, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Deep Dive into Baseline EMRQA Prediction Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed['gold_norm'] = failed['answer'].apply(norm)\n",
    "failed['pred_norm'] = failed['predicted_answer'].apply(norm)\n",
    "failed[\"clust_text\"] = failed.apply(\n",
    "    lambda r: f\"Q: {r['question']} GOLD: {r['gold_norm']} PRED: {r['pred_norm']}\",\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Tfidf + k-means did not yield meaningful clusters here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=3000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(failed[\"clust_text\"])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "kmeans = KMeans(n_clusters=k, random_state=0, n_init=10, max_iter=100)\n",
    "cluster_labels = kmeans.fit_predict(X)\n",
    "\n",
    "failed[\"cluster\"] = cluster_labels\n",
    "cluster_sizes = failed[\"cluster\"].value_counts().sort_index()\n",
    "cluster_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Per Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_summaries = {}\n",
    "\n",
    "for c in sorted(failed[\"cluster\"].unique()):\n",
    "    sub = failed[failed[\"cluster\"] == c]\n",
    "\n",
    "    qs = sub[\"question\"].astype(str).tolist()\n",
    "    if len(qs) < 5:\n",
    "        continue\n",
    "\n",
    "    cv = CountVectorizer(\n",
    "        max_features=2000,\n",
    "        ngram_range=(1,3),\n",
    "        stop_words=\"english\"\n",
    "    )\n",
    "    Xq = cv.fit_transform(qs)\n",
    "\n",
    "    sums = np.asarray(Xq.sum(axis=0)).ravel()\n",
    "    terms = np.array(cv.get_feature_names_out())\n",
    "    top_idx = sums.argsort()[::-1][:15]\n",
    "\n",
    "    cluster_summaries[c] = {\n",
    "        \"n\": len(sub),\n",
    "        \"top_terms\": list(terms[top_idx]),\n",
    "        \"error_dist\": sub[\"relation\"].value_counts().to_dict()\n",
    "    }\n",
    "\n",
    "cluster_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in sorted(cluster_summaries.keys()):\n",
    "    info = cluster_summaries[c]\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Cluster {c}  |  n = {info['n']}\")\n",
    "    print(\"Top terms:\", \", \".join(info[\"top_terms\"]))\n",
    "    print(\"Error types:\", info[\"error_dist\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = list(range(2, 25))\n",
    "sse = []\n",
    "\n",
    "for k in ks:\n",
    "    km = KMeans(n_clusters=k, random_state=0, n_init=10, max_iter=200)\n",
    "    km.fit(X)\n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "plt.plot(ks, sse, marker='o')\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Inertia (SSE)\")\n",
    "plt.title(\"Elbow Method\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Cluster error types in failed examples for further analysis. This uses sentence transformers to embed the question, context, and predicted/true answers, then clusters them using KMeans. This seems to work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "\n",
    "embed_texts = failed[\"clust_text\"].tolist()\n",
    "embeddings = model.encode(\n",
    "    embed_texts,\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, random_state=42)\n",
    "pca_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "failed[\"pca_x\"] = pca_2d[:, 0]\n",
    "failed[\"pca_y\"] = pca_2d[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = list(range(2, 16))\n",
    "sil_scores = []\n",
    "\n",
    "for k in ks:\n",
    "    km = KMeans(\n",
    "        n_clusters=k,\n",
    "        random_state=0,\n",
    "        n_init=10,\n",
    "        max_iter=300\n",
    "    )\n",
    "    labels = km.fit_predict(embeddings)\n",
    "    score = silhouette_score(embeddings, labels)\n",
    "    sil_scores.append(score)\n",
    "    print(f\"k={k:2d}  silhouette={score:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(ks, sil_scores, marker=\"o\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Silhouette score\")\n",
    "plt.title(\"Silhouette scores (KMeans on embeddings)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = 8\n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=best_k,\n",
    "    random_state=0,\n",
    "    n_init=10,\n",
    "    max_iter=300\n",
    ")\n",
    "k_labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "failed[\"cluster_kmeans\"] = k_labels\n",
    "failed[\"cluster_kmeans\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for cid in sorted(failed[\"cluster_kmeans\"].unique()):\n",
    "    mask = failed[\"cluster_kmeans\"] == cid\n",
    "    plt.scatter(\n",
    "        failed.loc[mask, \"pca_x\"],\n",
    "        failed.loc[mask, \"pca_y\"],\n",
    "        s=5,\n",
    "        alpha=0.7,\n",
    "        label=f\"cluster {cid}\"\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"PCA-1\")\n",
    "plt.ylabel(\"PCA-2\")\n",
    "plt.title(f\"KMeans (k={best_k}) on embeddings (PCA 2D)\")\n",
    "plt.legend(markerscale=3, bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_k_info = {}\n",
    "\n",
    "for cid in sorted(failed[\"cluster_kmeans\"].unique()):\n",
    "    sub = failed[failed[\"cluster_kmeans\"] == cid]\n",
    "    n = len(sub)\n",
    "    rel_dist = sub[\"relation\"].value_counts().to_dict()\n",
    "    \n",
    "    top_terms = []\n",
    "    if n >= 5:\n",
    "        cv = CountVectorizer(\n",
    "            max_features=2000,\n",
    "            ngram_range=(1,2),\n",
    "            stop_words=\"english\"\n",
    "        )\n",
    "        Xq = cv.fit_transform(sub[\"question\"].astype(str).tolist())\n",
    "        sums = np.asarray(Xq.sum(axis=0)).ravel()\n",
    "        terms = np.array(cv.get_feature_names_out())\n",
    "        top_idx = sums.argsort()[::-1][:15]\n",
    "        top_terms = list(terms[top_idx])\n",
    "    \n",
    "    cluster_k_info[cid] = {\n",
    "        \"n\": n,\n",
    "        \"relation_dist\": rel_dist,\n",
    "        \"top_terms\": top_terms,\n",
    "    }\n",
    "\n",
    "for cid, info in cluster_k_info.items():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Cluster {cid}  |  n = {info['n']}\")\n",
    "    print(\"Error-type distribution:\", info[\"relation_dist\"])\n",
    "    print(\"Top question terms:\", \", \".join(info[\"top_terms\"]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Manual Review of Sampled Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "- No. of questions in the train dataset that start with \"has\" is 62872/130,956\n",
    "- No. of questions in the validation dataset that start with \"has\" is 15913/32739\n",
    "- No. of questions in the failed examples that start with \"has\" is 1423/3248"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "This examines the whole dataset for train and eval to see how many questions start with \"has\", 'why' etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_dataset(\"Eladio/emrqa-msquad\", split=\"train\")\n",
    "\n",
    "# Filter questions that start with \"why\"\n",
    "why_questions = train.filter(lambda example: example[\"question\"].lower().startswith(\"why\"))\n",
    "\n",
    "why_questions_df = pd.DataFrame(why_questions)\n",
    "len(why_questions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = load_dataset(\"Eladio/emrqa-msquad\", split=\"validation\")\n",
    "\n",
    "# Filter questions that start with \"has\"\n",
    "has_questions = eval.filter(lambda example: example[\"question\"].lower().startswith(\"has\"))\n",
    "\n",
    "# Display the filtered examples\n",
    "has_questions_df = pd.DataFrame(has_questions)\n",
    "has_questions_df['question']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Insulin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "insulin_questions = failed[failed['question'].str.contains('insulin', case=False, na=False)]\n",
    "len(insulin_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Chest pain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "chest_pain_questions = failed[failed['question'].str.contains('chest pain', case=False, na=False)]\n",
    "len(chest_pain_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "Medication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "medication_questions = failed[failed['question'].str.contains('medication', case=False, na=False)]\n",
    "len(chest_pain_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# Generating the sample data from make_blobs\n",
    "# This particular setting has one distinct cluster and 3 clusters placed close\n",
    "# together.\n",
    "X, y = make_blobs(\n",
    "    n_samples=500,\n",
    "    n_features=2,\n",
    "    centers=4,\n",
    "    cluster_std=1,\n",
    "    center_box=(-10.0, 10.0),\n",
    "    shuffle=True,\n",
    "    random_state=1,\n",
    ")  # For reproducibility\n",
    "\n",
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(\n",
    "        X[:, 0], X[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(\n",
    "        centers[:, 0],\n",
    "        centers[:, 1],\n",
    "        marker=\"o\",\n",
    "        c=\"white\",\n",
    "        alpha=1,\n",
    "        s=200,\n",
    "        edgecolor=\"k\",\n",
    "    )\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n",
    "        % n_clusters,\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final-project (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
